{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect via ODBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import getpass\n",
    "\n",
    "OL_dsn = 'Olive_Prod'  # My DSN set up in ODBC manager \n",
    "OL_uid = 'thompsonja'\n",
    "OL_pwd = 'xxxxxx' #getpass.getpass('Enter Olive Password: ')\n",
    "OL_LOGIN = 'DSN='+ OL_dsn +';UID='+ OL_uid +';PWD=' + OL_pwd \n",
    "OL_conn = pyodbc.connect(OL_LOGIN, autocommit=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = OL_conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "......\n",
    "\"\"\"\n",
    "\n",
    "mycursor.execute(sql)\n",
    "mycursor.fetchall() # to fetch all results - not required for Create table etc\n",
    "mycursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_config_file(self):\n",
    "        \"\"\"\n",
    "        To avoid typing the password in everytime when logging into\n",
    "        Olive or Netezza, these are saved in a config file. Read this in.\n",
    "        \"\"\"        \n",
    "        self.db_config = {}\n",
    "        with open(self.db_config_loc) as f:\n",
    "            for line in f:\n",
    "               (key, val) = line.split()\n",
    "               self.db_config[key] = val\n",
    "\n",
    "\n",
    "    def olive_login(self):\n",
    "        \"\"\"\n",
    "        Login to Olive\n",
    "        \"\"\"\n",
    "        OL_dsn = 'Olive_Prod'  # My DSN set up in ODBC manager \n",
    "        OL_uid = 'thompsonja'\n",
    "        OL_pwd = self.db_config['Olive']\n",
    "        OL_LOGIN = 'DSN='+ OL_dsn +';UID='+ OL_uid +';PWD=' + OL_pwd \n",
    "        self.OL_conn = pyodbc.connect(OL_LOGIN, autocommit=True) \n",
    "        print(self.OL_conn.getinfo(pyodbc.SQL_DRIVER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert values into table: helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadValuesIntoTable(table_name, values_dic):\n",
    "    \"\"\"\n",
    "    Function to create a SQL string 'insert into table_name values (val1, val2, etc)' that can be executed.\n",
    "    table_name: the name of the table to insert the data into\n",
    "    values_dic: keys are the values to be inserted, if the values are True then treated as a string (i.e. '' added) \n",
    "                otherwise treated as a number\n",
    "    \n",
    "    Returns an sql string that can be executed\n",
    "    \"\"\"\n",
    "    values_str = ''\n",
    "    for k, v in values_dic.items():        \n",
    "        if v:\n",
    "            # string\n",
    "            value = \"\"\"'\"\"\" + k + \"\"\"', \"\"\"\n",
    "        else:\n",
    "            # number\n",
    "            value = str(k) + ', '\n",
    "        values_str += value\n",
    "    \n",
    "    values_str = values_str[:-2]\n",
    "    \n",
    "    sql = \"\"\" insert into {table_name} values ({values_str});\"\"\".format(table_name=table_name, values_str=values_str)  \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook = win32.Dispatch('outlook.application')\n",
    "mail = outlook.CreateItem(0)\n",
    "mail.To = 'jason.thompson@sky.uk; jonathan.green2@sky.uk'\n",
    "mail.Subject = 'Panel Expansion: 250k Data Ready'\n",
    "mail.Body = ''\n",
    "#mail.HTMLBody = '<h2>HTML Message body</h2>' #this field is optional\n",
    "\n",
    "# To attach a file to the email (optional):\n",
    "#attachment  = \"Path to the attachment\"\n",
    "#mail.Attachments.Add(attachment)\n",
    "\n",
    "mail.Send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all messages in subfldr_index which is a subfolder of inbox_index\n",
    "outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "inbox = outlook.GetDefaultFolder(6)\n",
    "subfldr = inbox.Folders[11]\n",
    "messages = subfldr.Items\n",
    "# identify most recent message\n",
    "dt = messages[0].ReceivedTime\n",
    "for message in messages:\n",
    "    if message.ReceivedTime >= dt:\n",
    "        dt = message.ReceivedTime\n",
    "        subject = message.Subject\n",
    "\n",
    "# check the most recent email\n",
    "if 'ENABLED' in subject:\n",
    "    return True\n",
    "else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise the values in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_value_analysis(table_df):\n",
    "    \"\"\"\n",
    "    Go through each column of table_df and summarise:\n",
    "        1. the number of unique values\n",
    "        2. the value with the highest count\n",
    "        3. the count of this value\n",
    "        4. the % of rows with this value\n",
    "\n",
    "    Return resuts as a dataframe\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    column_names = list(table_df)\n",
    "    column_summaries = []        \n",
    "\n",
    "    for i in range(table_df.shape[1]):\n",
    "        num_of_unique_values = table_df.iloc[:, i].nunique()\n",
    "        if num_of_unique_values > 0:\n",
    "            count_values = pd.DataFrame(table_df.groupby(column_names[i])[column_names[i]].count()).rename(mapper={column_names[i] : 'count'}, axis=1)\n",
    "            sort_values = count_values.sort_values(['count'], ascending=False)\n",
    "            column_summary = [column_names[i], num_of_unique_values, sort_values.index.values[0], sort_values.iloc[0, 0]]\n",
    "            column_summaries.append(column_summary)\n",
    "        else:\n",
    "            column_summary = [column_names[i], num_of_unique_values, -1, -1]\n",
    "\n",
    "\n",
    "    column_summaries_df = pd.DataFrame(column_summaries, columns=['Column Name', 'Unique Values', \n",
    "                                                                  'Value With Highest Count', 'Highest Count'])\n",
    "\n",
    "    column_summaries_df['Highest %'] = 100 * column_summaries_df['Highest Count'] / table_df.shape[0]\n",
    "    column_summaries_df = column_summaries_df.sort_values(['Highest %'], ascending=False)        \n",
    "\n",
    "    return column_summaries_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_create_table(sql, client, dataset, \n",
    "                    table_name, truncate=True):\n",
    "    \"\"\"\n",
    "    Create a table in BigQuery from the result of the SQL\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # set up table creation details to hold query results\n",
    "    create_instruction = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "    if truncate:            \n",
    "        write_instruction = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "    else:\n",
    "        write_instruction = bigquery.job.WriteDisposition.WRITE_APPEND\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.create_disposition = create_instruction\n",
    "    job_config.write_disposition = write_instruction\n",
    "    job_config.destination = dataset.table(table_name)\n",
    "\n",
    "    # Run the Query \n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "    # execute the query\n",
    "    iterator = query_job.result()  # Wait for job to complete\n",
    "    job_result = query_job.state         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Name of All Files in a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "\n",
    "os.chdir(barb_dir)    \n",
    "\n",
    "all_files = []    \n",
    "for (dirpath, dirnames, filenames) in walk('.'):\n",
    "    all_files.extend(filenames)\n",
    "    break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "timeshift_start = (datetime.datetime.strptime('2018-10-05', \"%Y-%m-%d\") - timedelta(days=3)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SBV_DATE'] = pd.to_datetime(df['SBV_DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the date or hour from a datetime\n",
    "\n",
    "live_df['capping_silo_date'] = live_df['start_hr'].dt.date\n",
    "live_df['hour'] = live_df['start_hr'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_parts_dic = {'Morning' : ('06:00:00', '11:59:59'),\n",
    "                 'Pre-Peak' : ('12:00:00', '17:24:59'),\n",
    "                 'Early Peak' : ('17:25:00', '19:59:59'),\n",
    "                 'Late Peak' : ('20:00:00', '22:59:59'),\n",
    "                 'Post Peak' : ('23:00:00', '00:29:59'),\n",
    "                 'Night Time' : ('00:30:00', '05:59:59')}\n",
    "\n",
    "\n",
    "day_part_lst = []\n",
    "process_date_range = pd.date_range(date_range[0], date_range[1]) \n",
    "for process_day in process_date_range:\n",
    "    for key, value in day_parts_dic.items():\n",
    "        start_time = datetime.datetime.strptime(value[0], \"%H:%M:%S\").time()\n",
    "        end_time = datetime.datetime.strptime(value[1], \"%H:%M:%S\").time()\n",
    "        \n",
    "        start_datetime = datetime.datetime.combine(process_day, start_time)\n",
    "        end_datetime = datetime.datetime.combine(process_day, end_time)\n",
    "        \n",
    "        if end_datetime < start_datetime:\n",
    "            end_datetime += datetime.timedelta(days=1)        \n",
    "        \n",
    "        day_part_lst.append([key, start_datetime, end_datetime])\n",
    "        \n",
    "\n",
    "day_part_df = pd.DataFrame(day_part_lst)\n",
    "\n",
    "day_part_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "df['match_value'] = df.apply(lambda row: fuzz.ratio(row['gcp_programme_name'], \n",
    "                                                    row['ol_programme_name']), axis=1)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts\n",
    "\n",
    "Useful blog: https://jakevdp.github.io/PythonDataScienceHandbook/04.10-customizing-ticks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "today_dt = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(return_df['SKY_PLUS'], linewidth=2, label='Sky+', color='b')\n",
    "plt.plot(return_df['SKY_Q'], linewidth=2, label='Sky Q', color='g')\n",
    "plt.plot(return_df['TOTAL'], linewidth=4, label='All', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "fig.suptitle('INC2897270: ROI Account Returns [{}]'.format(today_dt), fontsize=30)\n",
    "plt.ylabel('Number of Accounts', fontsize=25)\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.grid()\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "fig.savefig(\"INC2897270 ROI Account Returns.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "today_dt = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "plt.suptitle('Number of Accounts Watching BT Sports', fontsize=30)\n",
    "plt.title('Produced ' + today_dt, fontsize=18)\n",
    "plt.ylabel('Number of Accounts', fontsize=25)\n",
    "plt.xlabel('', fontsize=0, color='w')\n",
    "\n",
    "plt.tick_params(labelsize=20)\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "combined_df.plot(x='viewing_date', y='dist_acc', ax=ax, kind='line', label='Daily unique', color='b')\n",
    "combined_df.plot(x='viewing_date', y='rolling28_count', ax=ax, kind='line', label='28 day rolling', color='g')\n",
    "combined_df.plot(x='viewing_date', y='rolling56_count', ax=ax, kind='line', label='56 day rolling', color='r')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "today_dt = datetime.datetime.today().strftime(\"%Y%m%d\")\n",
    "if chart_path == '':\n",
    "    save_chart = os.getcwd() + '\\\\'\n",
    "else:\n",
    "    save_chart = chart_path + '\\\\' \n",
    "\n",
    "save_chart += 'Ocean BT Sports ' + today_dt + '.png'\n",
    "fig.savefig(save_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_format_large_numbers(value, tick_number):\n",
    "    if value >= 10**9:\n",
    "        return '{:.0f}'.format(value / 10**9)\n",
    "    elif value >= 10*6:\n",
    "        return '{:.0f}'.format(value / 10**6)\n",
    "    elif value >= 10*3:\n",
    "        return '{:.0f}'.format(value / 10**6)\n",
    "    elif value < 1:\n",
    "        return '{:.0%}'.format(value)\n",
    "    else:\n",
    "        return '{:.0f}'.format(value / 10**0)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(17, 14))\n",
    "\n",
    "xtick_freq = 3\n",
    "\n",
    "sub_ax = ax[0, 0]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_total_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Total (capped & scaled)')\n",
    "ol_month_df.plot(y='p11_total_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n",
    "\n",
    "\n",
    "sub_ax = ax[0, 1]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_rec_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Recorded (capped & scaled)')\n",
    "ol_month_df.plot(y='p11_rec_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n",
    "\n",
    "\n",
    "sub_ax = ax[1, 0]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_total_cap_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Total (capped NOT scaled)')\n",
    "ol_month_df.plot(y='p11_total_cap_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n",
    "\n",
    "\n",
    "sub_ax = ax[1, 1]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_rec_cap_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Recorded (capped NOT scaled)')\n",
    "ol_month_df.plot(y='p11_rec_cap_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n",
    "\n",
    "\n",
    "sub_ax = ax[2, 0]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_total_uncap_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Total (raw)')\n",
    "ol_month_df.plot(y='p11_total_uncap_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n",
    "\n",
    "\n",
    "sub_ax = ax[2, 1]\n",
    "sub_ax.tick_params(labelsize=10)\n",
    "sub_ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "sub_ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ol_month_df.plot(y='p10_rec_uncap_avgmins', ax=sub_ax, kind='line', label='Sky Q', color='b', title='Avg Mins Recorded (raw)')\n",
    "ol_month_df.plot(y='p11_rec_uncap_avgmins', ax=sub_ax, kind='line', label='Sky +', color='g')\n",
    "sub_ax.xaxis.set_label_text('', fontsize=0, color='w')\n",
    "sub_ax.set_xticks(range(len(list(ol_month_df.index.values)))[::xtick_freq])\n",
    "sub_ax.set_xticklabels(list(ol_month_df.index.values)[::xtick_freq])\n",
    "#sub_ax.yaxis.set_major_formatter(plt.FuncFormatter(chart_format_large_numbers))\n",
    "sub_ax.set_ylabel('Minutes', fontsize=10)\n",
    "sub_ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spyder inline etc\n",
    "\n",
    "from: https://stackoverflow.com/questions/29356269/plot-inline-or-a-separate-window-using-matplotlib-in-spyder-ide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# dispplay in console\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# display in seperate window\n",
    "get_ipython().run_line_magic('matplotlib', 'qt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr, p_val = pearsonr(df['Deselected Rate'], df['Market Share'])\n",
    "\n",
    "print('Correlation: {:.3f}; p-value: {:.3f}'.format(corr, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"test\">my_url</a>\n"
     ]
    }
   ],
   "source": [
    "my_url='my_url'\n",
    "\n",
    "\n",
    "print('<a href=\"{not_my_url}\">{}</a>'.format(my_url, my_url, not_my_url='test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiindex pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiindex_pivot(df, columns=None, values=None):\n",
    "    #https://github.com/pandas-dev/pandas/issues/23955\n",
    "    names = list(df.index.names)\n",
    "    df = df.reset_index()\n",
    "    list_index = df[names].values\n",
    "    tuples_index = [tuple(i) for i in list_index] # hashable\n",
    "    df = df.assign(tuples_index=tuples_index)\n",
    "    df = df.pivot(index=\"tuples_index\", columns=columns, values=values)\n",
    "    tuples_index = df.index  # reduced\n",
    "    index = pd.MultiIndex.from_tuples(tuples_index, names=names)\n",
    "    df.index = index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brexitOnly(df):\n",
    "    brexit_df = df.groupby(by=['mont', 'brexit_status'], as_index=False).sum()\n",
    "    brexit_df = brexit_df.pivot(index='mont', columns='brexit_status')\n",
    "    return brexit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_postcode(postcode):\n",
    "    \"\"\"Return a normalised postcode if valid, or None if not.\"\"\"\n",
    "    import re\n",
    "\n",
    "    NON_ALPHA_RE = re.compile('[^A-Z0-9]+')\n",
    "    POSTCODE_RE = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? [0-9][A-Z]{2}$')\n",
    "    \n",
    "    postcode = NON_ALPHA_RE.sub('', postcode.upper())\n",
    "    postcode = postcode[:-3] + ' ' + postcode[-3:]\n",
    "    if POSTCODE_RE.match(postcode):\n",
    "        return postcode\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_postcode(string):\n",
    "    \"\"\"From: https://stackoverflow.com/questions/378157/python-regular-expression-postcode-search\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # custom\n",
    "    method1 = re.findall(r'\\b[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][ABD-HJLNP-UW-Z]{2}\\b', string)\n",
    "    \n",
    "    #regex from #http://en.wikipedia.orgwikiUK_postcodes#Validation                                                                                            \n",
    "    method2 = re.findall(r'[A-Z]{1,2}[0-9R][0-9A-Z]? [0-9][A-Z]{2}', string)    \n",
    "    \n",
    "    return list(set(method1 + method2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
